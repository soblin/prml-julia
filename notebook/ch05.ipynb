{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "if isfile(\"../Project.toml\") && isfile(\"../Manifest.toml\")\n",
    "    Pkg.activate(\"..\");\n",
    "end\n",
    "\n",
    "using Random\n",
    "using Distributions\n",
    "using Plots\n",
    "using StatsPlots\n",
    "using prml\n",
    "using prml: pdf\n",
    "\n",
    "Random.seed!(1234);\n",
    "rng = MersenneTwister(1234);\n",
    "gr();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function create_toy_data(func, sample_size, domain=[0.0, 1.0], noise=0.1)\n",
    "    x = collect(range(domain[1], stop=domain[2], length=sample_size))\n",
    "    shuffle!(rng, x);\n",
    "    noise = rand(Uniform(-noise, noise), sample_size);\n",
    "    return x, func.(x) + noise\n",
    "end\n",
    "\n",
    "function func(x)\n",
    "    2.0 * x + 0.7 * sin(2 * pi * x)\n",
    "end\n",
    "\n",
    "function square(x)\n",
    "    return x^2\n",
    "end\n",
    "\n",
    "function sinusoidal(x)\n",
    "    return sin(pi * x)\n",
    "end\n",
    "\n",
    "function absolute(x)\n",
    "    if x >= 0.0\n",
    "        return x\n",
    "    else\n",
    "        return -x\n",
    "    end\n",
    "end\n",
    "\n",
    "function heaviside(x)\n",
    "    sign_x = x / absolute(x)\n",
    "    return 0.5 * (sign_x + 1.0)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagation\n",
    "\n",
    "Each node transforms input as follows.\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x} \\in \\mathbb{R}^{\\mathrm{input}} \\longrightarrow \\boldsymbol{z} = W \\boldsymbol{x} + \\boldsymbol{b} \\in \\mathbb{R}^{\\mathrm{output}} \\longrightarrow \\boldsymbol{y} = f(\\boldsymbol{z}) \\in \\mathbb{R}^{\\mathrm{output}}\n",
    "$$\n",
    "\n",
    "The inputs and outputs may consist of several samples, i.e. $\\boldsymbol{x} = [\\boldsymbol{x}_1 \\boldsymbol{x}_2 \\cdots \\boldsymbol{x}_N]$, $\\boldsymbol{z} = [\\boldsymbol{z}_1 \\boldsymbol{z}_2 \\cdots \\boldsymbol{z}_N]$. But the size of samples does not affect the update process for $W, \\boldsymbol{b}$.\n",
    "\n",
    "So $W \\in \\mathbb{R}^{\\mathrm{output} \\times \\mathrm{input}}$. In forward propagation, each node stores the values of $\\boldsymbol{x}$ and $\\boldsymbol{z}$.\n",
    "\n",
    "Then given $\\partial E / \\partial \\boldsymbol{y}$ at the end,\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\dfrac{\\partial E}{\\partial \\boldsymbol{z}} &= \\dfrac{\\partial E}{\\partial \\boldsymbol{y}} \\odot f^{\\prime}(\\boldsymbol{z}) = \\left[ \\dfrac{\\partial E}{\\partial \\boldsymbol{z}_1}, \\dfrac{\\partial E}{\\partial \\boldsymbol{z}_2}, \\cdots, \\dfrac{\\partial E}{\\partial \\boldsymbol{z}_N} \\right] \\in \\mathbb{R}^{\\mathrm{output}} \\\\\n",
    "\\dfrac{\\partial E}{\\partial W} &= \\dfrac{\\partial E}{\\partial \\boldsymbol{z}} \\boldsymbol{x}^{\\text{T}} \\in \\mathbb{R}^{\\mathrm{output}} \\times \\mathbb{R}^{\\mathrm{input}} \\\\\n",
    "\\dfrac{\\partial E}{\\partial \\boldsymbol{b}} &= \\sum_{i} \\dfrac{\\partial E}{\\partial \\boldsymbol{z}_i} \\in \\mathbb{R}^{\\mathrm{output}} \\\\\n",
    "\\dfrac{\\partial E}{\\partial \\boldsymbol{x}} &= \\boldsymbol{W}^{\\text{T}} f^{\\prime}(\\boldsymbol{z}) \\in \\mathbb{R}^{\\mathrm{input}}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "### SGD\n",
    "\n",
    "$$\\begin{align}\n",
    "\\boldsymbol{g}_t &= \\nabla E(\\boldsymbol{w}_t) \\\\\n",
    "\\Delta \\boldsymbol{w}_t &= -\\eta \\boldsymbol{g}_t \\\\\n",
    "\\boldsymbol{w}_{t+1} &= \\boldsymbol{w}_t + \\Delta \\boldsymbol{w}_t\n",
    "\\end{align}$$\n",
    "\n",
    "### Momentum SGD\n",
    "\n",
    "$$\\begin{align}\n",
    "\\boldsymbol{g}_t &= \\nabla E(\\boldsymbol{w}_t) \\\\\n",
    "\\Delta \\boldsymbol{w}_t &= \\mu \\Delta \\boldsymbol{w}_{t-1} - (1 - \\mu) \\eta \\boldsymbol{g}_t \\\\\n",
    "\\boldsymbol{w}_{t+1} &= \\boldsymbol{w}_t + \\Delta \\boldsymbol{w}_t\n",
    "\\end{align}$$\n",
    "\n",
    "### RMSProp\n",
    "\n",
    "$$\\begin{align}\n",
    "\\boldsymbol{g}_t &= \\nabla E(\\boldsymbol{w}_t) \\\\\n",
    "\\boldsymbol{v}_t &= \\rho \\boldsymbol{v}_{t-1} + (1 - \\rho) \\boldsymbol{g}^{2}_{t} \\\\\n",
    "\\Delta \\boldsymbol{w}_t &= -\\dfrac{\\eta}{\\sqrt{\\boldsymbol{v}_t + \\epsilon}} \\boldsymbol{g}_t \\\\\n",
    "\\boldsymbol{w}_{t+1} &= \\boldsymbol{w}_t + \\Delta \\boldsymbol{w}_t\n",
    "\\end{align}$$\n",
    "\n",
    "### Adam\n",
    "\n",
    "$$\\begin{align}\n",
    "\\boldsymbol{g}_t &= \\nabla E(\\boldsymbol{w}_t) \\\\\n",
    "\\boldsymbol{m}_t &= \\rho_1 \\boldsymbol{m}_{t-1} + (1 - \\rho_1) \\boldsymbol{g}_{t} \\\\\n",
    "\\boldsymbol{v}_t &= \\rho_2 \\boldsymbol{v}_{t-1} + (1 - \\rho_2) \\boldsymbol{g}^{2}_{t} \\\\\n",
    "\\hat{\\boldsymbol{m}}_t &= \\dfrac{\\boldsymbol{m}_t}{1 - \\rho_1} \\\\\n",
    "\\hat{\\boldsymbol{v}}_t &= \\dfrac{\\boldsymbol{v}_t}{1 - \\rho_2} \\\\\n",
    "\\Delta \\boldsymbol{w}_t &= -\\dfrac{\\eta}{\\sqrt{\\hat{\\boldsymbol{v}}_t + \\epsilon}} \\hat{\\boldsymbol{m}}_t \\\\\n",
    "\\boldsymbol{w}_{t+1} &= \\boldsymbol{w}_t + \\Delta \\boldsymbol{w}_t\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [TanhLayer(1, 4), TanhLayer(4, 3), LinearLayer(3, 1)];\n",
    "cost_function = SumSquareError();\n",
    "nn = NeuralNetwork(layers, cost_function);\n",
    "\n",
    "toy_functions = [square, sinusoidal, absolute, heaviside];\n",
    "domain = [-1.0, 1.0];\n",
    "\n",
    "plots = [];\n",
    "for toy_func in toy_functions\n",
    "    x_train, y_train = create_toy_data(toy_func, 100, domain, 0.01);\n",
    "    X = collect(reshape(x_train, 1, 100));\n",
    "    t = collect(reshape(y_train, 1, 100));\n",
    "\n",
    "    for i in 1:30000\n",
    "        X = collect(reshape(x_train, 1, 100));\n",
    "        t = collect(reshape(y_train, 1, 100));\n",
    "    \n",
    "        fitting(nn, X, t, 0.001);\n",
    "    end\n",
    "\n",
    "    x_test = collect(reshape(range(domain[1], domain[2], length=100), 1, 100));\n",
    "    X_test = copy(x_test);\n",
    "    X_test = predict(nn, X_test);\n",
    "\n",
    "    x_test = collect(reshape(x_test, 100));\n",
    "    X_test = collect(reshape(X_test, 100));\n",
    "    p = plot(x_train, y_train, seriestype=:scatter, alpha=0.5);\n",
    "    p = plot!(x_test, X_test, legend=nothing, lw=2, color=\"red\");\n",
    "    push!(plots, p);\n",
    "end\n",
    "\n",
    "plot(plots[1], plots[2], plots[3], plots[4], layout=(2, 2), size=(600, 500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ch5/image1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification with neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: [x, y] âˆˆ [-1, 1] x [-1, 1] --> output: {0, 1}\n",
    "## Data generation\n",
    "n_samples = 100\n",
    "points = rand(2, n_samples)\n",
    "points = points .* 2 .- 1.0\n",
    "\n",
    "function boundary(x)\n",
    "    return 1.2 .* x .* x\n",
    "end\n",
    "\n",
    "targets = [boundary(points[1, i]) < points[2, i] for i in 1:n_samples] * 1.0\n",
    "\n",
    "p = plot()\n",
    "x_test = collect(range(-1.0, stop=1.0, length=100));\n",
    "p = plot!(x_test, boundary(x_test), style=:dashdot)\n",
    "for i in 1:n_samples\n",
    "    if targets[i] == 1.0\n",
    "        p = scatter!([points[1, i]], [points[2, i]], seriestype=:scatter, alpha=0.5, color=\"red\")\n",
    "    else\n",
    "        p = scatter!([points[1, i]], [points[2, i]], seriestype=:scatter, alpha=0.5, color=\"blue\")\n",
    "    end\n",
    "end\n",
    "\n",
    "## Classification\n",
    "layers = [TanhLayer(2, 2), SigmoidLayer(2, 1)];\n",
    "cost_function = SigmoidCrossEntropy()\n",
    "targets = collect(reshape(targets, 1, n_samples))\n",
    "nn = NeuralNetwork(layers, cost_function)\n",
    "\n",
    "### Training\n",
    "n_training = 5000\n",
    "for i in 1:n_training\n",
    "    fitting(nn, points, targets, 0.001)\n",
    "end\n",
    "\n",
    "### Contour of nn prediction\n",
    "x = collect(range(-1.0, stop=1.0, length=10));\n",
    "y = collect(range(-1.0, stop=1.0, length=10));\n",
    "val = [predict(nn, reshape([i, j], 2, 1))[1] for j in y, i in x];\n",
    "p = contour!(x, y, val, vel_value=0.5)\n",
    "plot(p, xlims=(-1, 1), ylims=(-1, 1), legend=nothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ch5/image3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization of neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gen_toy_data(n=10)\n",
    "    x = collect(range(0.0, stop=1.0, length=n));\n",
    "    y = sin.(2 * pi * x) + rand(Normal(0, 0.25), n)\n",
    "    return x, y\n",
    "end\n",
    "\n",
    "x_train, y_train = gen_toy_data(10);\n",
    "plots = [];\n",
    "for i in [1, 3, 30]\n",
    "    X = collect(reshape(x_train, 1, 10));\n",
    "    t = collect(reshape(y_train, 1, 10));\n",
    "    layers = [TanhLayer(1, i), LinearLayer(i, 1)];\n",
    "    cost_function = SumSquareError();\n",
    "    nn = NeuralNetwork(layers, cost_function);\n",
    "    for _ in 1:40000\n",
    "        fitting(nn, X, t, 0.01);\n",
    "    end\n",
    "    \n",
    "    X_test = collect(reshape(range(0.0, stop=1.0, length=100), 1, 100));\n",
    "    t_test = predict(nn, X_test);\n",
    "    X_test = collect(reshape(X_test, 100));\n",
    "    t_test = collect(reshape(t_test, 100));\n",
    "    p = plot(X_test, t_test, lw=2, color=\"red\");\n",
    "    p = plot!(x_train, y_train, seriestype=:scatter);\n",
    "    push!(plots, p);\n",
    "end\n",
    "plot(plots[1], plots[2], plots[3], layout=(1, 3), size=(300*3, 250*1), legend=nothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ch5/image2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture density network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "function toy_func(x)\n",
    "    return x .+ 0.3 * sin.(2 * pi * x)\n",
    "end\n",
    "#x_lin = collect(reshape(range(0.0, stop=1.0, length=100), 100))\n",
    "x_train = collect(reshape(rand(n_samples), 1, n_samples))\n",
    "y_train = toy_func(x_train) .+ rand(Uniform(-0.08, 0.08), 1, n_samples)\n",
    "y_train = collect(reshape(y_train, 1, n_samples))\n",
    "\n",
    "# forward\n",
    "## Regression\n",
    "layers = [TanhLayer(1, 6), LinearLayer(6, 1)];\n",
    "cost_function = SumSquareError()\n",
    "nn = NeuralNetwork(layers, cost_function)\n",
    "for i in 1:5000\n",
    "    fitting(nn, x_train, y_train, 0.001)\n",
    "end\n",
    "\n",
    "plots = []\n",
    "x_test = collect(reshape(range(0, 1.0, length=100), 1, 100));\n",
    "y_test = predict(nn, x_test)\n",
    "x_test = collect(reshape(x_test, 100))\n",
    "y_test = collect(reshape(y_test, 100))\n",
    "p = plot(x_train, y_train, seriestype=:scatter, legend=nothing, color=\"green\")\n",
    "p = plot!(x_test, y_test, lw=2, color=\"red\")\n",
    "push!(plots, p)\n",
    "#p = plot!(x_lin, toy_func(x_lin), style=:dashdot)\n",
    "x_train, y_train = y_train, x_train\n",
    "nn = NeuralNetwork(layers, cost_function)\n",
    "for i in 1:5000\n",
    "    fitting(nn, x_train, y_train, 0.001)\n",
    "end\n",
    "x_test = collect(reshape(range(0, 1.0, length=100), 1, 100));\n",
    "y_test = predict(nn, x_test)\n",
    "x_test = collect(reshape(x_test, 100))\n",
    "y_test = collect(reshape(y_test, 100))\n",
    "p = plot(x_train, y_train, seriestype=:scatter, legend=nothing, color=\"green\")\n",
    "p = plot!(x_test, y_test, lw=2, color=\"red\")\n",
    "push!(plots, p)\n",
    "\n",
    "plot(plots[1], plots[2], layout=(1, 2), size=(300*2, 250*1), legend=nothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ch5/image4.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not working.\n",
    "function mixture_gaussian(x::Float64, mus::AbstractArray{Float64, 1}, sigma2s::AbstractArray{Float64, 1}, pis::AbstractArray{Float64, 1})\n",
    "    ret = 0.0\n",
    "    for k in 1:size(mus)[1]\n",
    "        ret += pis[k] * exp(-(x - mus[k])^2 / (2 * sigma2s[k])) / sqrt(2.0 * pi * sigma2s[k])\n",
    "    end\n",
    "    return ret\n",
    "end\n",
    "\n",
    "function sample(x::AbstractArray{Float64, 2}, n_samples)\n",
    "    x_size = size(x)[2]\n",
    "    indices = rand(1:x_size, n_samples)\n",
    "    return x[:, indices]\n",
    "end\n",
    "\n",
    "layers = [TanhLayer(1, 5), LinearLayer(5, 9)];\n",
    "mixture_function = GaussianMixtureError(3);\n",
    "nn = NeuralNetwork(layers, mixture_function);\n",
    "learning_rate = 0.001\n",
    "for i in 1:100000\n",
    "    X = sample(x_train, 70)\n",
    "    t = sample(y_train, 70)\n",
    "    for layer in nn.layers\n",
    "        X = forward_propagation(layer, X)\n",
    "    end\n",
    "    \n",
    "    diff = delta(mixture_function, X, t)\n",
    "    for layer in reverse(nn.layers)\n",
    "        diff = backward_propagation(layer, diff, learning_rate)\n",
    "    end\n",
    "end\n",
    "\n",
    "x_test = collect(reshape(range(0, 1.0, length=100), 1, 100));\n",
    "output = copy(x_test)\n",
    "for layer in nn.layers\n",
    "    output = forward_propagation(layer, output)\n",
    "end\n",
    "\n",
    "for n in 1:100\n",
    "    pis = view(output, 1:3, n)\n",
    "    mus = view(output, 4:6, n)\n",
    "    sigma2s = view(output, 7:9, n)\n",
    "    \n",
    "    # (1) convert output to [pis, mus, sigma2s]\n",
    "    # pis\n",
    "    max_pi = maximum(pis)\n",
    "    for k in 1:3\n",
    "        pis[k] = pis[k] - max_pi\n",
    "    end\n",
    "    for k in 1:3\n",
    "        pis[k] = exp(pis[k])\n",
    "    end\n",
    "    sum_pis = sum(pis)\n",
    "    for k in 1:3\n",
    "        pis[k] /= sum_pis\n",
    "    end\n",
    "    # sigma2s\n",
    "    for k in 1:3\n",
    "        sigma2s[k] = exp(sigma2s[k])^2\n",
    "    end\n",
    "end\n",
    "\n",
    "y_test = [mixture_gaussian(x_test[1, i], output[4:6, i], output[7:9, i], output[1:3, i]) for i in 1:100]\n",
    "x_test = collect(reshape(x_test, 100))\n",
    "y_test = collect(reshape(y_test, 100))\n",
    "plot(x_test, y_test, lw=2, legend=nothing, color=\"red\")\n",
    "plot!(x_train[1, :], y_train[1, :], seriestype=:scatter, legend=nothing, color=\"green\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
