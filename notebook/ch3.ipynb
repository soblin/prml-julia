{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "if isfile(\"../Project.toml\") && isfile(\"../Manifest.toml\")\n",
    "    Pkg.activate(\"..\");\n",
    "end\n",
    "\n",
    "using Random\n",
    "using Distributions\n",
    "using Plots\n",
    "using StatsPlots\n",
    "using prml\n",
    "using prml: pdf\n",
    "\n",
    "Random.seed!(1234);\n",
    "rng = MersenneTwister(1234);\n",
    "gr();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function create_toy_data(func, sample_size, std, domain=[0., 1.])\n",
    "    x = collect(range(domain[1], stop=domain[2], length=sample_size));\n",
    "    shuffle!(rng, x);\n",
    "    noise = rand(Normal(0.0, std), sample_size);\n",
    "    return x, func(x) + noise\n",
    "end\n",
    "\n",
    "function sinusoidal(x)\n",
    "    return sin.(2 * pi * x)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial, Gaussian, Sigmoid features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = collect(range(-1.0, stop=1.0, length=100));\n",
    "means = collect(range(-1.0, stop=1.0, length=12));\n",
    "feature_poly = PolynomialFeature(11);\n",
    "feature_gauss = GaussianFeature(means, 0.1);\n",
    "feature_sigmoid = SigmoidalFeature(means, 10.0);\n",
    "\n",
    "X_polynomial = transform(feature_poly, x);\n",
    "X_gaussian = transform(feature_gauss, x);\n",
    "X_sigmoidal = transform(feature_sigmoid, x);\n",
    "\n",
    "# polynomial\n",
    "p1 = plot();\n",
    "for j in 1:12\n",
    "    p1 = plot!(x, X_polynomial[:, j]);\n",
    "end\n",
    "# gaussian\n",
    "p2 = plot();\n",
    "for j in 1:12\n",
    "    p2 = plot!(x, X_gaussian[:, j]);\n",
    "end\n",
    "# sigmoidal\n",
    "p3 = plot();\n",
    "for j in 1:12\n",
    "    p3 = plot!(x, X_sigmoidal[:, j]);\n",
    "end\n",
    "plot(p1, p2, legend=nothing)\n",
    "# plot(p1, p2, p3, legend=nothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ch3/image1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression\n",
    "\n",
    "$$\n",
    "\\boldsymbol{w} = (\\boldsymbol{\\Phi}^{T} \\boldsymbol{\\Phi} + \\lambda I)^{-1} \\boldsymbol{\\Phi}^{T} \\boldsymbol{t}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = create_toy_data(sinusoidal, 10, 0.4);\n",
    "x_test = collect(range(0, stop=1.0, length=100));\n",
    "y_test = sinusoidal(x_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = collect(range(0, stop=1.0, length=7));\n",
    "feature = GaussianFeature(means, 0.1);\n",
    "\n",
    "X_train = transform(feature, x_train);\n",
    "X_test = transform(feature, x_test);\n",
    "model = LinearRegressor([0], 0);\n",
    "fitting(model, X_train, y_train);\n",
    "\n",
    "y, var = predict(model, X_test, true);\n",
    "y_std = fill(var, size(y)[1]);\n",
    "\n",
    "plot(x_test, y_test, label=\"sin(2π x)\", color=\"steelblue\", lw=2);\n",
    "plot!(x_train, y_train, label=\"training data\", seriestype=:scatter);\n",
    "plot!(x_test, y, ribbon=(y_std, y_std), fillalpha=0.5, label=\"std\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ch3/image2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RidgeRegressor([0.0], 1e-2);\n",
    "fitting(model, X_train, y_train);\n",
    "y = predict(model, X_test);\n",
    "\n",
    "plot(x_train, y_train, label=\"training data\", seriestype=:scatter);\n",
    "plot!(x_test, y, label=\"predict\", lw=2)\n",
    "plot!(x_test, y_test, label=\"sin(2π x)\", lw=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ch3/image3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = collect(range(0, stop=1.0, length=24));\n",
    "feature = GaussianFeature(means, 0.1);\n",
    "\n",
    "plots = [];\n",
    "for a in [1e2, 1.0, 1e-9]\n",
    "    y_list = [];\n",
    "    p = plot();\n",
    "    for i in 1:100\n",
    "        x_train, y_train = create_toy_data(sinusoidal, 25, 0.25);\n",
    "        x_test = collect(range(0, stop=1.0, length=100));\n",
    "        X_train = transform(feature, x_train);\n",
    "        X_test = transform(feature, x_test);\n",
    "        model = BayesianRegressor(a, 1.0, 24);\n",
    "        fitting(model, X_train, y_train);\n",
    "        y = predict(model, X_test);\n",
    "        push!(y_list, y);\n",
    "        if i < 20\n",
    "            p = plot!(x_test, y, color=\"orange\");\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    push!(plots, p);\n",
    "    p = plot();\n",
    "    p = plot!(x_test, y_test, color=\"steelblue\", lw=2);\n",
    "    p = plot!(x_test, mean(y_list), color=\"orange\", lw=2);\n",
    "    push!(plots, p);\n",
    "end\n",
    "\n",
    "plot(plots[1], plots[2], plots[3], plots[4], plots[5], plots[6], layout=(3, 2), ylim=(-1.5, 1.5), legend=nothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ch3/image4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function linear(x)\n",
    "    return -0.3 .+ 0.5 .* x\n",
    "end\n",
    "\n",
    "x_train, y_train = create_toy_data(linear, 20, 0.1, [-1., 1.]);\n",
    "x = collect(range(-1., stop=1., length=100));\n",
    "y = collect(range(-1., stop=1., length=100));\n",
    "#grids = [[i, j] for i in x, j in y];\n",
    "\n",
    "model = BayesianRegressor(1.0, 100.0, 2);\n",
    "feature = PolynomialFeature(1);\n",
    "X_train = transform(feature, x_train);\n",
    "X = transform(feature, x);\n",
    "\n",
    "dist = MvNormal(model._w_mean, inv(model._w_precision));\n",
    "vals = [Distributions.pdf(dist, [i, j]) for j in y, i in x];\n",
    "\n",
    "plots = [];\n",
    "p = contour(x, y, vals, colorbar=false);\n",
    "p = plot!([-0.3], [0.5], seriestype=:scatter, marker=:dot, markersize=:4, markercolor=\"blue\");\n",
    "push!(plots, p);\n",
    "\n",
    "Y_random = predictSampling(model, X, 6);\n",
    "p = plot();\n",
    "for i in 1:6\n",
    "    p = plot!(x, Y_random[i, :], c=\"orange\");\n",
    "end\n",
    "push!(plots, p);\n",
    "\n",
    "for (index, values) in enumerate([[1, 1], [2, 2], [3, 3], [4, 20]])\n",
    "    first = values[1];\n",
    "    last = values[2];\n",
    "    fitting(model, X_train[first:last, :], y_train[first:last]);\n",
    "    dist = MvNormal(model._w_mean, inv(model._w_precision));\n",
    "    vals = [Distributions.pdf(dist, [i, j]) for j in y, i in x];\n",
    "    p = contour(x, y, vals, colorbar=false);\n",
    "    p = plot!([-0.3], [0.5], seriestype=:scatter, marker=:dot, markersize=:4, markercolor=\"blue\");\n",
    "    push!(plots, p);\n",
    "\n",
    "    Y_random = predictSampling(model, X, 6);\n",
    "    p = plot();\n",
    "    for i in 1:6\n",
    "        p = plot!(x, Y_random[i, :], c=\"orange\");\n",
    "    end\n",
    "    p = plot!(x_train[1:last], y_train[1:last], c=\"steelblue\", seriestype=:scatter);\n",
    "    push!(plots, p);\n",
    "end\n",
    "\n",
    "plot(plots[1], plots[2], plots[3], plots[4], plots[5], plots[6], plots[7], plots[8], plots[9], plots[10], layout=(5, 2), size=(600, 1250), legend=nothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ch3/image5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian linear regression\n",
    "\n",
    "### Regression\n",
    "\n",
    "The data is generated from the distribution\n",
    "\n",
    "$$\n",
    "\\begin{align} t = y(\\boldsymbol{x}, \\boldsymbol{w}) + \\epsilon \\end{align}.\n",
    "$$\n",
    "\n",
    "Or\n",
    "\n",
    "$$\n",
    "\\begin{align} p(t \\mid \\boldsymbol{w}, \\beta) = \\mathcal{N}(t \\mid y(\\boldsymbol{x}, \\boldsymbol{w}), \\beta^{-1}) \\end{align}\n",
    "$$\n",
    "\n",
    "And the training data is denoted as $\\boldsymbol{X} = [\\boldsymbol{x}_1, \\cdots \\boldsymbol{x}_N]$, and $\\boldsymbol{t} = [t_1, \\cdots t_N]$. The likelihood for the traning data is then given by\n",
    "\n",
    "$$\n",
    "\\begin{align} p(\\boldsymbol{t} \\mid \\boldsymbol{X}, \\boldsymbol{w}, \\beta) = \\prod \\mathcal{N}(t_n \\mid \\boldsymbol{w}^{\\text{T}} \\boldsymbol{\\phi}(\\boldsymbol{x}_n), \\beta^{-1}) \\end{align}\n",
    "$$\n",
    "\n",
    "Now we assume the prior distribution for $\\boldsymbol{w}$ is $\\mathcal{N}(\\boldsymbol{w} \\mid \\boldsymbol{m}_0, S_0)$. Then the Bayes's theorem gives the posterior distribution for $\\boldsymbol{w}$ as follows,\n",
    "\n",
    "$$\n",
    "\\begin{align} p(\\boldsymbol{w} \\mid \\boldsymbol{t}_N) &= \\mathcal{N}(\\boldsymbol{m}_N, S_N) \\\\ \\boldsymbol{m}_N &= S_N(S_0^{-1} \\boldsymbol{m}_0 + \\beta \\Phi_N^{\\text{T}}\\boldsymbol{t}_N) \\\\ S_N^{-1} &= S_0^{-1} + \\beta \\Phi_N^{\\text{T}}\\Phi_N \\end{align}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\Phi_{N}^{\\text{T}} = \\begin{bmatrix} \\boldsymbol{\\phi}_{1} & \\boldsymbol{\\phi}_{2} & \\cdots & \\boldsymbol{\\phi}_{N} \\end{bmatrix}, \\quad\n",
    "\\Phi_N = \\begin{bmatrix} \\boldsymbol{\\phi}_{1}^{\\text{T}} \\\\ \\boldsymbol{\\phi}_{2}^{\\text{T}} \\\\ \\vdots \\\\ \\boldsymbol{\\phi}_{N}^{\\text{T}} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Predictive distribution\n",
    "\n",
    "$$\n",
    "p(t \\mid \\boldsymbol{x}, \\alpha, \\beta) = \\mathcal{N} \\left(t \\mid\\boldsymbol{m}_{N}^{\\text{T}}\\phi(\\boldsymbol{x}), \\sigma^2(\\boldsymbol{x}) \\right)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\sigma^2(\\boldsymbol{x}) = \\dfrac{1}{\\beta} + \\phi(\\boldsymbol{x})^{\\text{T}} S_N \\phi(\\boldsymbol{x})\n",
    "$$\n",
    "\n",
    "As you can see, the variance is dependent on $\\boldsymbol{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = create_toy_data(sinusoidal, 25, 0.25);\n",
    "x_test = collect(range(0, stop=1.0, length=100));\n",
    "y_test = sinusoidal(x_test);\n",
    "\n",
    "means = collect(range(0, stop=1.0, length=9));\n",
    "feature = GaussianFeature(means, 0.1);\n",
    "X_train = transform(feature, x_train);\n",
    "X_test = transform(feature, x_test);\n",
    "\n",
    "model = BayesianRegressor(1e-3, 2.0, 9);\n",
    "plots = [];\n",
    "\n",
    "for (index, ranges) in enumerate([[1, 1], [2, 2], [3, 4], [5, 9], [10, 15], [16, 25]])\n",
    "    first, last = ranges[1], ranges[2];\n",
    "    fitting(model, X_train[first:last, :], y_train[first:last]);\n",
    "    y, y_std = predict(model, X_test, true);\n",
    "    p = plot(x_train[1:last], y_train[1:last], seriestype=:scatter);\n",
    "    p = plot!(x_test, y_test, lw=2, color=\"steelblue\");\n",
    "    p = plot!(x_test, y, ribbon=(y_std, y_std), fillalpha=0.5, color=\"orange\", lw=2);\n",
    "    push!(plots, p);\n",
    "end\n",
    "\n",
    "l = @layout [a b; c d; e f];\n",
    "\n",
    "plot(plots[1], plots[2], plots[3], plots[4], plots[5], plots[6], xlim=(0, 1), ylim=(-2, 2), layout=l, legend=nothing, size=(600, 750))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ch3/image6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evidence approximiation\n",
    "\n",
    "$$\\begin{align}\n",
    "\\ln p(\\boldsymbol{t} \\mid \\alpha, \\beta) &= \\dfrac{M}{2} \\ln \\alpha + \\dfrac{N}{2} \\ln \\beta - E(\\boldsymbol{w}) - \\dfrac{1}{2} \\ln \\det A - \\dfrac{N}{2} \\ln (2 \\pi) \\\\\n",
    "A &= \\alpha I + \\beta \\Phi^{\\text{T}}\\Phi \\\\\n",
    "E(\\boldsymbol{w}) &= \\dfrac{\\beta}{2} \\| \\boldsymbol{t} - \\Phi \\boldsymbol{w} \\| + \\dfrac{\\alpha}{2} \\boldsymbol{w}^{\\text{T}} \\boldsymbol{w}\n",
    "\\end{align}$$\n",
    "\n",
    "Minimization of $\\ln p(\\boldsymbol{t} \\mid \\alpha, \\beta)$ is achieved as follows.\n",
    "\n",
    "1. $\\alpha \\leftarrow \\alpha_0$ \n",
    "2. $\\beta \\leftarrow \\beta_0$ \n",
    "3. **while**  $(\\alpha, \\beta)$ **converges do**:\n",
    "4.  $\\boldsymbol{m}_N \\leftarrow \\beta(\\alpha I + \\beta \\Phi^{\\text{T}}\\Phi)^{-1}\\Phi^{\\text{T}}\\boldsymbol{t}$ \n",
    "5. $\\boldsymbol{\\lambda} \\leftarrow \\mathrm{eig}(\\beta \\Phi^{\\text{T}}\\Phi)$ \n",
    "6. $\\gamma \\leftarrow \\sum_i \\lambda_i / (\\alpha + \\lambda_i)$ \n",
    "7. $\\alpha \\leftarrow \\gamma / |\\boldsymbol{m}_N|$ \n",
    "8. $1 / \\beta \\leftarrow \\sum_{n=1}^{N}(\\boldsymbol{m}_N^{\\text{T}}\\boldsymbol{\\phi}(\\boldsymbol{x}_n) - t_n)^2 / (N - \\gamma)$ \n",
    "9. **end while**\n",
    "10. **return**  $(\\alpha, \\beta)$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function cubic(x)\n",
    "    return x .* (x .- 5.0) .* (x .+ 5.0)\n",
    "end\n",
    "\n",
    "x_train, y_train = create_toy_data(cubic, 30, 10, [-5.0, 5.0]);\n",
    "x_test = collect(range(-5.0, stop=5.0, length=100));\n",
    "evidences = [];\n",
    "models = [];\n",
    "\n",
    "for i in 1:6\n",
    "    feature = PolynomialFeature(i);\n",
    "    X_train = transform(feature, x_train);\n",
    "    model = EmpiricalBayesianRegressor(100.0, 100.0, i+1);\n",
    "    fitting(model, X_train, y_train, 100);\n",
    "    push!(evidences, log_evidence(model, X_train, y_train));\n",
    "    push!(models, model);\n",
    "end\n",
    "\n",
    "plot(evidences, xlabel=\"degree\", ylabel=\"log evidence\", title=\"Model Evidence\", legend=nothing, lw=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ch3/image7.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "degree = findall(isequal(minimum(evidences)), evidences);\n",
    "regressor = models[degree[1]];\n",
    "feature = PolynomialFeature(degree[1]);\n",
    "\n",
    "X_test = transform(feature, x_test);\n",
    "y, y_std = predict(regressor, X_test, true);\n",
    "plot(x_test, cubic(x_test), label=\"x(x-5)(x+5)\", lw=2)\n",
    "plot!(x_test, y, ribbon=(y_std, y_std), label=\"prediction\", fillalpha=0.5, color=\"orange\", lw=2)\n",
    "plot!(x_train, y_train, seriestype=:scatter, label=\"training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ch3/image8.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
